{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "# Add the src directory to Python path\n",
    "olmo_core_path = Path.cwd() / \"src\"\n",
    "if olmo_core_path.exists():\n",
    "    sys.path.insert(0, str(olmo_core_path))\n",
    "\n",
    "from olmo_core.data import (\n",
    "    NumpyDataLoaderConfig,\n",
    "    NumpyDatasetConfig,\n",
    "    NumpyDatasetType,\n",
    "    TokenizerConfig,\n",
    ")\n",
    "from olmo_core.data.numpy_dataset import (\n",
    "    VSLCurriculumType,\n",
    "    VSLCurriculumConfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joberant/NLP_2425b/yoavbaron/anaconda3/envs/ai2-olmo-2-copy/lib/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set your new cache base directory (change this to your preferred location)\n",
    "cache_base = \"/home/joberant/NLP_2425b/yoavbaron\"\n",
    "\n",
    "# Set all relevant Hugging Face cache directories\n",
    "os.environ[\"HF_HOME\"] = cache_base\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(cache_base, \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(cache_base, \"datasets\")\n",
    "os.environ[\"HF_TOKENIZERS_CACHE\"] = os.path.join(cache_base, \"tokenizers\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from olmo_eval import HFTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = TokenizerConfig.dolma2()\n",
    "tokenizer = HFTokenizer(\n",
    "            tokenizer_config.identifier,\n",
    "            pad_token_id=tokenizer_config.pad_token_id,\n",
    "            eos_token_id=tokenizer_config.eos_token_id,\n",
    "            bos_token_id=tokenizer_config.bos_token_id,\n",
    "        )\n",
    "\n",
    "include_instance_metadata = False # Set to true when you want tp retrieve metadata, during training set this to False\n",
    "work_dir = \"/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/hp_final/dataset-cache\"\n",
    "\n",
    "with open(os.path.join(cache_base, \"knowledge-analysis-suite/OLMo-core/grouped_dict.pkl\"), \"rb\") as f:\n",
    "    swapping_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading metadata:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading metadata: 100%|██████████| 8/8 [00:00<00:00, 121.98it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_config = NumpyDatasetConfig.glob(\n",
    "    \"/home/morg/students/gottesman3/knowledge-analysis-suite/dolma/python/final_tokenizations_with_offsets/no_special/*.npy\",  # can be globs\n",
    "    name=NumpyDatasetType.kas_vsl,\n",
    "    max_sequence_length=2048,\n",
    "    min_sequence_length=64,\n",
    "    vsl_curriculum=VSLCurriculumConfig(name=VSLCurriculumType.grow_p2, num_cycles=8, balanced=False),\n",
    "    tokenizer=tokenizer_config,\n",
    "    work_dir=str(work_dir),\n",
    "    include_instance_metadata=include_instance_metadata,\n",
    "    swapping_dict=swapping_dict\n",
    ")\n",
    "kas_dataset = dataset_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<olmo_core.data.numpy_dataset.NumpyKASVSLDataset at 0x7392783e6e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_config = NumpyDataLoaderConfig(\n",
    "    global_batch_size=32768,\n",
    "    seed=0,\n",
    "    num_workers=8,\n",
    "    prefetch_factor = 16,\n",
    ")\n",
    "\n",
    "dataloader = data_loader_config.build(kas_dataset)\n",
    "dataloader.reshuffle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NumpyVSLDataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataloader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NumpyVSLDataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "dataloader[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to swap chunk 2465503\n",
      "Swapping chunk 2465503 with 6401103\n",
      "Trying to swap chunk 2715621\n",
      "Trying to swap chunk 564478Swapping chunk 2715621 with 4775495\n",
      "\n",
      "Swapping chunk 564478 with 7690370\n",
      "Trying to swap chunk 10046982\n",
      "Swapping chunk 10046982 with 1142565\n",
      "Trying to swap chunk 3945203\n",
      "Swapping chunk 3945203 with 4458969\n",
      "Trying to swap chunk 5149756\n",
      "Swapping chunk 5149756 with 7307548\n",
      "Trying to swap chunk 564233\n",
      "Swapping chunk 564233 with 1400467\n",
      "Trying to swap chunk 7442658\n",
      "Swapping chunk 7442658 with 3266546\n",
      "dict_keys(['input_ids', 'attention_mask', 'index'])\n",
      "17 tensor([ 3626703,  7216007,  9127750,  4308706,  3561431,  4620958,  7592063,\n",
      "        10195124, 10161708,  3590584,  7534629,  3471291,  6353848,  2465503,\n",
      "         4114366,  5607351,  1918612])\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i.keys())\n",
    "    print(len(i['index']), i['index'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NumpyVSLDataLoader' object is not an iterator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m next_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NumpyVSLDataLoader' object is not an iterator"
     ]
    }
   ],
   "source": [
    "next_item = next(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PopQA dataset and filter entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_chunks(dataset, min_num_chunks, max_num_chunks, instance_lengths):\n",
    "    # Filter the dataset\n",
    "    filtered_dataset = dataset['train'].filter(\n",
    "        lambda example: min_num_chunks <= example['subject_num_chunks'] <= max_num_chunks\n",
    "    )   \n",
    "\n",
    "    # Create list of dictionaries with subject info and chunk lengths\n",
    "    result_list = []    \n",
    "\n",
    "    for example in filtered_dataset:\n",
    "        \n",
    "        subject_name = example['subj']\n",
    "        subject_id = example['subj_id']\n",
    "        chunks = example['subject_chunks']\n",
    "        num_chunks = example['subject_num_chunks']\n",
    "\n",
    "        chunk_lengths = instance_lengths[chunks]\n",
    "\n",
    "        if subject_name == 'Madison':\n",
    "            print(chunks)\n",
    "\n",
    "        # Sort chunks by their lengths (descending order)\n",
    "        if len(chunk_lengths) > 0:\n",
    "            # Create pairs of (chunk, length) and sort by length\n",
    "            chunk_length_pairs = list(zip(chunks, chunk_lengths))\n",
    "            chunk_length_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Separate back into sorted chunks and lengths\n",
    "            sorted_chunks = [pair[0] for pair in chunk_length_pairs]\n",
    "            sorted_lengths = [pair[1] for pair in chunk_length_pairs]\n",
    "        else:\n",
    "            sorted_chunks = chunks\n",
    "            sorted_lengths = chunk_lengths\n",
    "\n",
    "        subject_dict = {\n",
    "            'entity_id': subject_id,\n",
    "            'num_chunks': num_chunks,\n",
    "            'chunks': sorted_chunks,\n",
    "            'chunks_lengths': sorted_lengths\n",
    "        }\n",
    "        \n",
    "        result_list.append(subject_dict)    \n",
    "\n",
    "    # Sort the list by number of chunks (descending order)\n",
    "    result_list.sort(key=lambda x: x['num_chunks'], reverse=True)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[820777, 941125, 976593, 1132074, 4955122, 5784515, 5893462, 6285853, 6301590, 6569641, 6709763, 982625, 1069094, 1174705, 1175438, 1188820, 3892745, 6043687, 7585232, 2052326, 4449953, 6471684, 6415190, 263910, 430203, 436392, 857271, 964356, 1492452, 1751563, 1953559, 2146414, 2244409, 2359129, 2390139, 2799444, 2839387, 3210372, 3220392, 3475484, 3514601, 3586220, 3735351, 3892746, 3950691, 4321806, 4489791, 4814079, 5279548, 5384395, 5985105, 6007311, 6123047, 6773572, 6837529, 6855838, 7299974, 7309924, 7714134, 8089661, 9309412, 9423790, 9496875, 9866817, 4171091, 8267712, 3130262, 8233434, 1211449, 6231806, 9180749, 342090, 534038, 1647807, 1961324, 2725096, 3113130, 3123297, 3461098, 4020853, 4255747, 4622325, 4713269, 4976107, 6497698, 6498978, 6855755, 7551764, 7688290, 7901132, 9352219, 10391979, 7551765, 7551766, 879397, 3232229, 7994593, 10329160, 7585233]\n",
      "[500753, 500754, 500756, 7899334, 9766174, 7586543, 3930628, 4385903, 5956585, 5902917, 7143593, 8695947, 1058648, 376536, 44139, 497690, 502027, 538702, 834173, 1010163, 1064626, 1111175, 1261840, 1481121, 1614061, 3221993, 3879948, 3941380, 4690429, 4812317, 5088880, 5368533, 5424424, 5468647, 5775020, 5898955, 5924327, 5927015, 6556267, 7053728, 7306906, 7379354, 7565395, 7602136, 8536409, 8681558, 9398657, 10184736, 10364904, 10432436, 721997, 1182170, 6230463, 3884369, 3930629, 9945605, 105907, 434180, 524611, 1905795, 1905796, 2140438, 2180729, 2309454, 2771081, 3089355, 3096097, 3151176, 3185012, 3551953, 3667924, 4253970, 5182149, 6067698, 6198859, 6226373, 6495923, 7852436, 8539391, 8549811, 8779233, 9820170, 10468777, 3884370]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"dhgottesman/popqa-kas\")\n",
    "\n",
    "\"\"\"\n",
    "importsnt chunks has the following structure:\n",
    "        {\n",
    "            'entity_id': subject_id,\n",
    "            'num_chunks': num_chunks,\n",
    "            'chunks': sorted_chunks,\n",
    "            'chunks_lengths': sorted_lengths\n",
    "        }\n",
    "\"\"\"\n",
    "important_chunks = get_important_chunks(ds, 50, 100, kas_dataset.get_instance_lengths())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load original batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = np.load(\"/home/morg/students/gottesman3/knowledge-analysis-suite/OLMo-core/batch_indices.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample injection points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_injection_points(total_steps, num_points_to_sample, max_num_chunks, interval, seed=None):\n",
    "    \"\"\"\n",
    "    Samples unique injection points from a valid starting range to avoid overflow \n",
    "    when assigning chunk indices.\n",
    "\n",
    "    Args:\n",
    "        total_steps (int): The maximum possible step value (exclusive upper bound).\n",
    "        num_points_to_sample (int): Number of injection points to sample.\n",
    "        max_num_chunks (int): Maximum num_chunks across all entities.\n",
    "        interval (int): Distance between chunk indices.\n",
    "        seed (int, optional): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: Sorted list of valid injection starting points.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    max_valid_start = total_steps - (max_num_chunks - 1) * interval\n",
    "    if max_valid_start <= 0:\n",
    "        raise ValueError(\"Interval and chunk size too large for total steps.\")\n",
    "\n",
    "    if num_points_to_sample > max_valid_start:\n",
    "        raise ValueError(\"Cannot sample more injection points than available valid start points.\")\n",
    "\n",
    "    sampled_points = random.sample(range(max_valid_start), k=num_points_to_sample)\n",
    "    return sorted(sampled_points)\n",
    "\n",
    "\n",
    "def assign_indices_to_entities(entities, injection_points, interval):\n",
    "    \"\"\"\n",
    "    Assigns indices to each entity starting at a given injection point with spacing.\n",
    "\n",
    "    Args:\n",
    "        entities (List[dict]): List of entity dicts.\n",
    "        injection_points (List[int]): List of sampled injection start points.\n",
    "        interval (int): Distance between chunk indices.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[int]]: Mapping from entity name to list of indices.\n",
    "    \"\"\"\n",
    "    if len(entities) != len(injection_points):\n",
    "        raise ValueError(\"Number of entities must match number of injection points.\")\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for entity, start in zip(entities, injection_points):\n",
    "        entity_id = entity['entity_id']\n",
    "        num_chunks = entity['num_chunks']\n",
    "        indices = [start + i * interval for i in range(num_chunks)]\n",
    "        result[entity_id] = indices\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 5\n",
    "\n",
    "total_number_of_batches = dataloader.total_batches\n",
    "injection_points = sample_injection_points(total_number_of_batches, len(important_chunks), 100, interval, 0)\n",
    "all_injection_points_per_entity = assign_indices_to_entities(important_chunks, injection_points, interval)\n",
    "#all_injection_points_per_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Swapping Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shloop(\n",
    "    injection_points: List[int],\n",
    "    entity_data: dict,\n",
    "    batch_to_chunks_map: dict,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # 1. Get entity chunks available for swapping and their lengths\n",
    "\n",
    "    ent_chunk_to_len = dict(zip(entity_data['chunks'], entity_data['chunks_lengths']))\n",
    "    ent_len_to_chunk = {v: k for k, v in ent_chunk_to_len.items()}\n",
    "\n",
    "    # casting to int but might want to edit this\n",
    "    batch_id_to_len = {}\n",
    "    batch_len_to_id = {}\n",
    "    for batch in injection_points:\n",
    "        batch_len = int(32768 / len(batch_to_chunks_map[batch]))\n",
    "        batch_id_to_len[batch] = batch_len\n",
    "        batch_len_to_id[batch_len] = batch\n",
    "\n",
    "    # 2. Calculate the injection span\n",
    "    num_chunks = len(entity_data['chunks'])\n",
    "    #print(f\"Injection span: {list(injection_points)}\")\n",
    "    if len(injection_points) != num_chunks:\n",
    "        f\"Entity {entity_data['entity_id']} expected {num_chunks} injection points, but got {len(injection_points)}.\"\n",
    "    \n",
    "\n",
    "    sb = sorted(batch_len_to_id.keys())   \n",
    "    se = sorted(ent_len_to_chunk.keys())\n",
    "\n",
    "    chunks_to_batches = []\n",
    "    for len_e in se:\n",
    "        for len_b in sb:\n",
    "            if len_b == len_e:\n",
    "                #print(len_e, len_b)\n",
    "                chunk_id = ent_len_to_chunk[len_e]\n",
    "                batch_id = batch_len_to_id[len_b]\n",
    "\n",
    "                #print(f\"Chunk {chunk_id} with length {len_e} will be swapped with batch {batch_id} with length {len_b}\")\n",
    "                # get a random chunk id from the batch\n",
    "                chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "\n",
    "                if [chunk_id, chunk_id_from_batch] in chunks_to_batches or [chunk_id_from_batch, chunk_id] in chunks_to_batches:\n",
    "                    print(chunk_id, chunk_id_from_batch, \"already in\")\n",
    "                \n",
    "                chunks_to_batches.append([chunk_id, chunk_id_from_batch])\n",
    "                chunks_to_batches.append([chunk_id_from_batch, chunk_id])\n",
    "                #chunks_to_batches[chunk_id] = chunk_id_from_batch # chunk e goes to chunk e' in batch b\n",
    "                #chunks_to_batches[chunk_id_from_batch] = chunk_id # add the symetric mapping\n",
    "\n",
    "                ent_len_to_chunk.pop(len_e) # pop one of the lengths\n",
    "                ent_chunk_to_len.pop(chunk_id) # pop the chunk from the entity and pop one of the lengths\n",
    "                batch_len_to_id.pop(len_b)\n",
    "                batch_id_to_len.pop(batch_id) # pop the batch and the length from the batch\n",
    "                break\n",
    "                \n",
    "    # ranmly match the rest of the chunks\n",
    "    for chunk_id, batch_id in zip(ent_chunk_to_len.keys(), batch_id_to_len.keys()):\n",
    "        if chunk_id not in chunks_to_batches:\n",
    "            chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "\n",
    "            if [chunk_id, chunk_id_from_batch] in chunks_to_batches or [chunk_id_from_batch, chunk_id] in chunks_to_batches:\n",
    "                    print(chunk_id, chunk_id_from_batch, \"already in\")\n",
    "                \n",
    "            chunks_to_batches.append([chunk_id, chunk_id_from_batch])\n",
    "            chunks_to_batches.append([chunk_id_from_batch, chunk_id])\n",
    "            \n",
    "            #chunks_to_batches[chunk_id] = chunk_id_from_batch\n",
    "            #chunks_to_batches[chunk_id_from_batch] = chunk_id\n",
    "\n",
    "    return chunks_to_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mapping = []\n",
    "for i, important_chunk in enumerate(important_chunks):\n",
    "    pts = all_injection_points_per_entity[important_chunk['entity_id']]\n",
    "\n",
    "    # The 'important_chunk' variable is the integer you need.\n",
    "    # Pass it directly to your function.\n",
    "    res = shloop(\n",
    "        pts,\n",
    "        important_chunk,\n",
    "        all_batches\n",
    "    )\n",
    "    # extend full mapping with the result\n",
    "    full_mapping.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined length of all lists under important_chunks: 72433\n"
     ]
    }
   ],
   "source": [
    "combined_length = sum(entity['num_chunks'] for entity in important_chunks)\n",
    "print(\"Combined length of all lists under important_chunks:\", combined_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144866"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dict = {}\n",
    "for key, value in full_mapping:\n",
    "    if key not in grouped_dict:\n",
    "        grouped_dict[key] = []\n",
    "    grouped_dict[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(cache_base, \"knowledge-analysis-suite/OLMo-core/grouped_dict.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(grouped_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items appearing more than once at position 0: 12873\n",
      "Items appearing more than once at position 1: 12873\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Dictionary to track occurrences of each item at each position (0 or 1)\n",
    "position_counts = [defaultdict(int), defaultdict(int)]\n",
    "\n",
    "for pair in full_mapping:\n",
    "    position_counts[0][pair[0]] += 1\n",
    "    position_counts[1][pair[1]] += 1\n",
    "\n",
    "duplicates = {\n",
    "    0: [item for item, count in position_counts[0].items() if count > 1],\n",
    "    1: [item for item, count in position_counts[1].items() if count > 1]\n",
    "}\n",
    "\n",
    "print(\"Items appearing more than once at position 0:\", len(duplicates[0]))\n",
    "print(\"Items appearing more than once at position 1:\", len(duplicates[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunk ids appearing in multiple entities: 2534\n",
      "Chunk ID 10380177 appears in entities: [1258065, 2083978]\n",
      "Chunk ID 2189467 appears in entities: [1258065, 978452]\n",
      "Chunk ID 3066924 appears in entities: [1258065, 2083978]\n",
      "Chunk ID 10378761 appears in entities: [1258065, 2083978]\n",
      "Chunk ID 3277628 appears in entities: [1200906, 22940, 188071]\n",
      "Chunk ID 3373418 appears in entities: [22940, 188071]\n",
      "Chunk ID 3373419 appears in entities: [22940, 188071]\n",
      "Chunk ID 3507554 appears in entities: [2143529, 22940]\n",
      "Chunk ID 4460895 appears in entities: [22940, 188071]\n",
      "Chunk ID 9734793 appears in entities: [929961, 22940, 1203333]\n",
      "Chunk ID 3107368 appears in entities: [22940, 188071]\n",
      "Chunk ID 1560730 appears in entities: [744404, 22940]\n",
      "Chunk ID 3550098 appears in entities: [1774154, 22940, 188071]\n",
      "Chunk ID 4460896 appears in entities: [22940, 188071]\n",
      "Chunk ID 2661416 appears in entities: [2886218, 22940]\n",
      "Chunk ID 3395466 appears in entities: [723619, 22940]\n",
      "Chunk ID 8937698 appears in entities: [22940, 188071]\n",
      "Chunk ID 3524574 appears in entities: [1708415, 22940, 2903271]\n",
      "Chunk ID 4480865 appears in entities: [2448274, 22940]\n",
      "Chunk ID 3373421 appears in entities: [22940, 188071]\n",
      "Chunk ID 5711518 appears in entities: [22940, 188071]\n",
      "Chunk ID 3879754 appears in entities: [22940, 188071]\n",
      "Chunk ID 3516130 appears in entities: [218538, 22940]\n",
      "Chunk ID 1144003 appears in entities: [924491, 1227861]\n",
      "Chunk ID 1940511 appears in entities: [924491, 1227861]\n",
      "Chunk ID 7850924 appears in entities: [924491, 2164229]\n",
      "Chunk ID 7850925 appears in entities: [924491, 2164229]\n",
      "Chunk ID 4076146 appears in entities: [924491, 1227861]\n",
      "Chunk ID 4076143 appears in entities: [924491, 1227861]\n",
      "Chunk ID 1940514 appears in entities: [924491, 1227861]\n",
      "Chunk ID 1940513 appears in entities: [924491, 1227861]\n",
      "Chunk ID 672843 appears in entities: [924491, 1616301]\n",
      "Chunk ID 8247061 appears in entities: [731361, 924491]\n",
      "Chunk ID 8193791 appears in entities: [924491, 1227861]\n",
      "Chunk ID 198181 appears in entities: [924491, 2918711]\n",
      "Chunk ID 1835590 appears in entities: [924491, 1227861]\n",
      "Chunk ID 8247062 appears in entities: [731361, 924491]\n",
      "Chunk ID 4959066 appears in entities: [924491, 2164229]\n",
      "Chunk ID 8645159 appears in entities: [731361, 924491]\n",
      "Chunk ID 2891309 appears in entities: [145054, 913383]\n",
      "Chunk ID 3255810 appears in entities: [293872, 1055978, 145054, 913383]\n",
      "Chunk ID 2179829 appears in entities: [2930621, 913383]\n",
      "Chunk ID 2916723 appears in entities: [2828860, 145054, 913383]\n",
      "Chunk ID 2955126 appears in entities: [145054, 913383]\n",
      "Chunk ID 3050877 appears in entities: [145054, 913383]\n",
      "Chunk ID 3255809 appears in entities: [26052, 270854, 913383, 2576966, 1055978, 293872, 145054]\n",
      "Chunk ID 3666554 appears in entities: [2854401, 320554, 1926542, 913383]\n",
      "Chunk ID 3870875 appears in entities: [145054, 331942, 913383]\n",
      "Chunk ID 4409877 appears in entities: [2334061, 807934, 913383]\n",
      "Chunk ID 4416018 appears in entities: [927603, 913383]\n",
      "Chunk ID 1198534 appears in entities: [145054, 913383]\n",
      "Chunk ID 3713255 appears in entities: [1271828, 913383]\n",
      "Chunk ID 1225088 appears in entities: [1573598, 913383]\n",
      "Chunk ID 2328114 appears in entities: [145054, 913383]\n",
      "Chunk ID 3913782 appears in entities: [145054, 913383]\n",
      "Chunk ID 1198535 appears in entities: [145054, 913383]\n",
      "Chunk ID 4924892 appears in entities: [145054, 913383]\n",
      "Chunk ID 1523404 appears in entities: [145054, 913383]\n",
      "Chunk ID 4000674 appears in entities: [145054, 913383]\n",
      "Chunk ID 2670260 appears in entities: [1335996, 89510]\n",
      "Chunk ID 2865460 appears in entities: [2017243, 1335996, 1203333]\n",
      "Chunk ID 2947431 appears in entities: [2017243, 1335996, 1203333]\n",
      "Chunk ID 2986098 appears in entities: [1335996, 1140503]\n",
      "Chunk ID 3658536 appears in entities: [147257, 1335996]\n",
      "Chunk ID 3696117 appears in entities: [1335996, 1203333]\n",
      "Chunk ID 3892308 appears in entities: [2888874, 653466, 1335996]\n",
      "Chunk ID 9301223 appears in entities: [2017243, 1335996, 1203333]\n",
      "Chunk ID 10092690 appears in entities: [1335996, 89510]\n",
      "Chunk ID 3657682 appears in entities: [1335996, 2930621, 89510]\n",
      "Chunk ID 3713254 appears in entities: [1335996, 1271828]\n",
      "Chunk ID 2865465 appears in entities: [1335996, 1203333]\n",
      "Chunk ID 2865466 appears in entities: [2017243, 1335996, 1203333, 89510]\n",
      "Chunk ID 2865467 appears in entities: [1335996, 1203333]\n",
      "Chunk ID 3820268 appears in entities: [147257, 1335996]\n",
      "Chunk ID 2511139 appears in entities: [2033555, 1335996]\n",
      "Chunk ID 4546041 appears in entities: [1335996, 1203333]\n",
      "Chunk ID 5823351 appears in entities: [1335996, 145054]\n",
      "Chunk ID 3063697 appears in entities: [1395154, 1335996, 818732]\n",
      "Chunk ID 3456120 appears in entities: [2847596, 1335996]\n",
      "Chunk ID 3986398 appears in entities: [1226666, 1335996, 2930621]\n",
      "Chunk ID 8262702 appears in entities: [1901626, 1335996]\n",
      "Chunk ID 8144429 appears in entities: [1335996, 89510]\n",
      "Chunk ID 2556534 appears in entities: [1335996, 89510]\n",
      "Chunk ID 2271695 appears in entities: [1980435, 2082991]\n",
      "Chunk ID 1212116 appears in entities: [1980435, 572485]\n",
      "Chunk ID 2895409 appears in entities: [1885774, 37422]\n",
      "Chunk ID 2948743 appears in entities: [40312, 37422]\n",
      "Chunk ID 3615341 appears in entities: [2912753, 37422]\n",
      "Chunk ID 4773264 appears in entities: [1885774, 37422]\n",
      "Chunk ID 2948744 appears in entities: [1885774, 37422]\n",
      "Chunk ID 2903515 appears in entities: [40312, 37422]\n",
      "Chunk ID 3001896 appears in entities: [40312, 37422]\n",
      "Chunk ID 2965512 appears in entities: [40312, 1885774, 37422]\n",
      "Chunk ID 5069655 appears in entities: [1885774, 37422]\n",
      "Chunk ID 4356574 appears in entities: [40312, 37422]\n",
      "Chunk ID 5458031 appears in entities: [40312, 37422]\n",
      "Chunk ID 8680826 appears in entities: [1885774, 37422]\n",
      "Chunk ID 9481487 appears in entities: [1885774, 37422]\n",
      "Chunk ID 8244229 appears in entities: [40312, 37422]\n",
      "Chunk ID 1027375 appears in entities: [1885774, 37422]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Track chunk_id -> list of entity_ids where it appears\n",
    "chunk_to_entities = defaultdict(set)\n",
    "\n",
    "for entity in important_chunks:\n",
    "    entity_id = entity['entity_id']\n",
    "    for chunk_id in entity['chunks']:\n",
    "        chunk_to_entities[chunk_id].add(entity_id)\n",
    "\n",
    "# Find chunk_ids that appear in more than one entity\n",
    "duplicate_chunks = {chunk_id: list(entity_ids) for chunk_id, entity_ids in chunk_to_entities.items() if len(entity_ids) > 1}\n",
    "\n",
    "print(f\"Number of chunk ids appearing in multiple entities: {len(duplicate_chunks)}\")\n",
    "for chunk_id, entity_ids in list(duplicate_chunks.items())[:100]:  # show first 10 for brevity\n",
    "    print(f\"Chunk ID {chunk_id} appears in entities: {entity_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144866.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_len = 0\n",
    "for d in full_mapping:\n",
    "    total_len += len(d)\n",
    "\n",
    "total_len / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(swapping_dict, f)\n",
      "File \u001b[0;32m/home/joberant/NLP_2425b/yoavbaron/anaconda3/envs/ai2-olmo-2-copy/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl'"
     ]
    }
   ],
   "source": [
    "# with open('/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl', 'wb') as f:\n",
    "    # pickle.dump(swapping_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back later\n",
    "with open('/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl', 'rb') as f:\n",
    "    swapping_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild dataset and dataloader with swapped chunk indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading metadata: 100%|██████████| 8/8 [00:00<00:00, 279.79it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_config = TokenizerConfig.dolma2()\n",
    "tokenizer = HFTokenizer(\n",
    "            tokenizer_config.identifier,\n",
    "            pad_token_id=tokenizer_config.pad_token_id,\n",
    "            eos_token_id=tokenizer_config.eos_token_id,\n",
    "            bos_token_id=tokenizer_config.bos_token_id,\n",
    "        )\n",
    "\n",
    "include_instance_metadata = False # Set to true when you want tp retrieve metadata, during training set this to False\n",
    "work_dir = \"/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/hp_final/dataset-cache\"\n",
    "\n",
    "\n",
    "dataset_config = NumpyDatasetConfig.glob(\n",
    "    \"/home/morg/students/gottesman3/knowledge-analysis-suite/dolma/python/final_tokenizations_with_offsets/no_special/*.npy\",  # can be globs\n",
    "    name=NumpyDatasetType.kas_vsl,\n",
    "    max_sequence_length=2048,\n",
    "    min_sequence_length=64,\n",
    "    vsl_curriculum=VSLCurriculumConfig(name=VSLCurriculumType.grow_p2, num_cycles=8, balanced=False),\n",
    "    tokenizer=tokenizer_config,\n",
    "    work_dir=str(work_dir),\n",
    "    include_instance_metadata=include_instance_metadata,\n",
    "    swapping_dict = swapping_dict,\n",
    ")\n",
    "\n",
    "reordered_dataset = dataset_config.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_config = NumpyDataLoaderConfig(\n",
    "    global_batch_size=32768,\n",
    "    seed=0,\n",
    "    num_workers=8,\n",
    "    prefetch_factor = 16,\n",
    ")\n",
    "\n",
    "dataloader = data_loader_config.build(reordered_dataset)\n",
    "dataloader.reshuffle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = list(swapping_dict.keys())\n",
    "sorted_keys.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2-olmo-2-copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
