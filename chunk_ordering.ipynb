{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "# Add the src directory to Python path\n",
    "olmo_core_path = Path.cwd() / \"src\"\n",
    "if olmo_core_path.exists():\n",
    "    sys.path.insert(0, str(olmo_core_path))\n",
    "\n",
    "from olmo_core.data import (\n",
    "    NumpyDataLoaderConfig,\n",
    "    NumpyDatasetConfig,\n",
    "    NumpyDatasetType,\n",
    "    TokenizerConfig,\n",
    ")\n",
    "from olmo_core.data.numpy_dataset import (\n",
    "    VSLCurriculumType,\n",
    "    VSLCurriculumConfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your new cache base directory (change this to your preferred location)\n",
    "cache_base = \"/home/joberant/NLP_2425b/yoavbaron\"\n",
    "\n",
    "# Set all relevant Hugging Face cache directories\n",
    "os.environ[\"HF_HOME\"] = cache_base\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(cache_base, \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(cache_base, \"datasets\")\n",
    "os.environ[\"HF_TOKENIZERS_CACHE\"] = os.path.join(cache_base, \"tokenizers\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from olmo_eval import HFTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = TokenizerConfig.dolma2()\n",
    "tokenizer = HFTokenizer(\n",
    "            tokenizer_config.identifier,\n",
    "            pad_token_id=tokenizer_config.pad_token_id,\n",
    "            eos_token_id=tokenizer_config.eos_token_id,\n",
    "            bos_token_id=tokenizer_config.bos_token_id,\n",
    "        )\n",
    "\n",
    "include_instance_metadata = False # Set to true when you want tp retrieve metadata, during training set this to False\n",
    "work_dir = \"/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/hp_final/dataset-cache\"\n",
    "\n",
    "dataset_config = NumpyDatasetConfig.glob(\n",
    "    \"/home/morg/students/gottesman3/knowledge-analysis-suite/dolma/python/final_tokenizations_with_offsets/no_special/*.npy\",  # can be globs\n",
    "    name=NumpyDatasetType.kas_vsl,\n",
    "    max_sequence_length=2048,\n",
    "    min_sequence_length=64,\n",
    "    vsl_curriculum=VSLCurriculumConfig(name=VSLCurriculumType.grow_p2, num_cycles=8, balanced=False),\n",
    "    tokenizer=tokenizer_config,\n",
    "    work_dir=str(work_dir),\n",
    "    include_instance_metadata=include_instance_metadata,\n",
    ")\n",
    "kas_dataset = dataset_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_config = NumpyDataLoaderConfig(\n",
    "    global_batch_size=32768,\n",
    "    seed=0,\n",
    "    num_workers=8,\n",
    "    prefetch_factor = 16,\n",
    ")\n",
    "\n",
    "dataloader = data_loader_config.build(kas_dataset)\n",
    "dataloader.reshuffle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PopQA dataset and filter entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_chunks(dataset, min_num_chunks, max_num_chunks, instance_lengths):\n",
    "    # Filter the dataset\n",
    "    filtered_dataset = dataset['train'].filter(\n",
    "        lambda example: min_num_chunks <= example['subject_num_chunks'] <= max_num_chunks\n",
    "    )   \n",
    "\n",
    "    # Create list of dictionaries with subject info and chunk lengths\n",
    "    result_list = []    \n",
    "\n",
    "    for example in filtered_dataset:\n",
    "        \n",
    "        subject_name = example['subj']\n",
    "        subject_id = example['subj_id']\n",
    "        chunks = example['subject_chunks']\n",
    "        num_chunks = example['subject_num_chunks']\n",
    "\n",
    "        chunk_lengths = instance_lengths[chunks]\n",
    "\n",
    "        if subject_name == 'Madison':\n",
    "            print(chunks)\n",
    "\n",
    "        # Sort chunks by their lengths (descending order)\n",
    "        if len(chunk_lengths) > 0:\n",
    "            # Create pairs of (chunk, length) and sort by length\n",
    "            chunk_length_pairs = list(zip(chunks, chunk_lengths))\n",
    "            chunk_length_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Separate back into sorted chunks and lengths\n",
    "            sorted_chunks = [pair[0] for pair in chunk_length_pairs]\n",
    "            sorted_lengths = [pair[1] for pair in chunk_length_pairs]\n",
    "        else:\n",
    "            sorted_chunks = chunks\n",
    "            sorted_lengths = chunk_lengths\n",
    "\n",
    "        subject_dict = {\n",
    "            'entity_id': subject_id,\n",
    "            'entity_name': subject_name,\n",
    "            'num_chunks': num_chunks,\n",
    "            'chunks': sorted_chunks,\n",
    "            'chunks_lengths': sorted_lengths\n",
    "        }\n",
    "        \n",
    "        result_list.append(subject_dict)    \n",
    "\n",
    "    # Sort the list by number of chunks (descending order)\n",
    "    result_list.sort(key=lambda x: x['num_chunks'], reverse=True)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"dhgottesman/popqa-kas\")\n",
    "\n",
    "\"\"\"\n",
    "importsnt chunks has the following structure:\n",
    "        {\n",
    "            'entity_id': subject_id,\n",
    "            'num_chunks': num_chunks,\n",
    "            'chunks': sorted_chunks,\n",
    "            'chunks_lengths': sorted_lengths\n",
    "        }\n",
    "\"\"\"\n",
    "important_chunks = get_important_chunks(ds, 50, 100, kas_dataset.get_instance_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disjoint_entities(subject_dicts, seed = 406):\n",
    "    used_chunks = set()\n",
    "    disjoint_entities = []\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    shuffled = subject_dicts.copy()\n",
    "    random.shuffle(shuffled)\n",
    "\n",
    "    for entity in shuffled:\n",
    "        entity_chunks = set(entity['chunks'])\n",
    "\n",
    "        # Check if entity has any overlapping chunk\n",
    "        if used_chunks.isdisjoint(entity_chunks):\n",
    "            disjoint_entities.append(entity)\n",
    "            used_chunks.update(entity_chunks)\n",
    "\n",
    "    return disjoint_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seed = 0\n",
    "max_len = 0\n",
    "for i in range(5000):\n",
    "    current_len = len(get_disjoint_entities(important_chunks, i))\n",
    "    if current_len > max_len:\n",
    "        max_len = current_len\n",
    "        max_seed = i\n",
    "\n",
    "max_seed, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disjoint_entities = get_disjoint_entities(important_chunks, max_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_set = ()\n",
    "\n",
    "for entity in disjoint_entities:\n",
    "    for chunk in entity['chunks']:\n",
    "        if chunk in chunks_set:\n",
    "            print(\"not disjoint\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load original batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = np.load(\"/home/morg/students/gottesman3/knowledge-analysis-suite/OLMo-core/batch_indices.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample injection points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_injection_points(total_steps, num_points_to_sample, max_num_chunks, interval, seed=None):\n",
    "    \"\"\"\n",
    "    Samples unique injection points from a valid starting range to avoid overflow \n",
    "    when assigning chunk indices.\n",
    "\n",
    "    Args:\n",
    "        total_steps (int): The maximum possible step value (exclusive upper bound).\n",
    "        num_points_to_sample (int): Number of injection points to sample.\n",
    "        max_num_chunks (int): Maximum num_chunks across all entities.\n",
    "        interval (int): Distance between chunk indices.\n",
    "        seed (int, optional): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: Sorted list of valid injection starting points.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    max_valid_start = total_steps - (max_num_chunks - 1) * interval\n",
    "    if max_valid_start <= 0:\n",
    "        raise ValueError(\"Interval and chunk size too large for total steps.\")\n",
    "\n",
    "    if num_points_to_sample > max_valid_start:\n",
    "        raise ValueError(\"Cannot sample more injection points than available valid start points.\")\n",
    "\n",
    "    sampled_points = random.sample(range(max_valid_start), k=num_points_to_sample)\n",
    "    return sorted(sampled_points)\n",
    "\n",
    "\n",
    "def assign_indices_to_entities(entities, injection_points, interval):\n",
    "    \"\"\"\n",
    "    Assigns indices to each entity starting at a given injection point with spacing.\n",
    "\n",
    "    Args:\n",
    "        entities (List[dict]): List of entity dicts.\n",
    "        injection_points (List[int]): List of sampled injection start points.\n",
    "        interval (int): Distance between chunk indices.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[int]]: Mapping from entity name to list of indices.\n",
    "    \"\"\"\n",
    "    if len(entities) != len(injection_points):\n",
    "        raise ValueError(\"Number of entities must match number of injection points.\")\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for entity, start in zip(entities, injection_points):\n",
    "        entity_id = entity['entity_id']\n",
    "        num_chunks = entity['num_chunks']\n",
    "        indices = [start + i * interval for i in range(num_chunks)]\n",
    "        result[entity_id] = indices\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 1\n",
    "\n",
    "total_number_of_batches = dataloader.total_batches\n",
    "injection_points = sample_injection_points(total_number_of_batches, len(disjoint_entities), 100, interval, 0)\n",
    "all_injection_points_per_entity = assign_indices_to_entities(disjoint_entities, injection_points, interval)\n",
    "# all_injection_points_per_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "inj = sorted(itertools.chain.from_iterable(all_injection_points_per_entity.values()))[:10]\n",
    "print(inj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    if i == inj[0]:\n",
    "        print(len(batch['index']), batch['index'])\n",
    "        break\n",
    "    if i > max(inj):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Swapping Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def shloop(\n",
    "    injection_points: List[int],\n",
    "    entity_data: dict,\n",
    "    batch_to_chunks_map: dict,\n",
    "    blacklist = []\n",
    ") -> List[List]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # 1. Get entity chunks available for swapping and their lengths\n",
    "    fixed_lengths = [2 ** math.ceil(math.log2(length)) for length in entity_data['chunks_lengths']]\n",
    "    \n",
    "    ent_chunk_to_len = dict(zip(entity_data['chunks'], fixed_lengths))\n",
    "    ent_len_to_chunk = {v: k for k, v in ent_chunk_to_len.items()}\n",
    "\n",
    "    # casting to int but might want to edit this\n",
    "    batch_id_to_len = {}\n",
    "    batch_len_to_id = {}\n",
    "    for batch in injection_points:\n",
    "        batch_len = int(32768 / len(batch_to_chunks_map[batch]))\n",
    "        batch_id_to_len[batch] = batch_len\n",
    "        batch_len_to_id[batch_len] = batch\n",
    "\n",
    "    # 2. Calculate the injection span\n",
    "    num_chunks = len(entity_data['chunks'])\n",
    "    #print(f\"Injection span: {list(injection_points)}\")\n",
    "    if len(injection_points) != num_chunks:\n",
    "        f\"Entity {entity_data['entity_id']} expected {num_chunks} injection points, but got {len(injection_points)}.\"\n",
    "    \n",
    "\n",
    "    sb = sorted(batch_len_to_id.keys())   \n",
    "    se = sorted(ent_len_to_chunk.keys())\n",
    "    #bucket_length = 2 ** math.ceil(math.log2(length))\n",
    "    chunks_to_batches = []\n",
    "    for len_e in se:\n",
    "        for len_b in sb:\n",
    "            if len_b == len_e:\n",
    "                #print(len_e, len_b)\n",
    "                chunk_id = ent_len_to_chunk[len_e]\n",
    "                batch_id = batch_len_to_id[len_b]\n",
    "\n",
    "                #print(f\"Chunk {chunk_id} with length {len_e} will be swapped with batch {batch_id} with length {len_b}\")\n",
    "                # get a random chunk id from the batch\n",
    "                chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "                while chunk_id_from_batch in blacklist:\n",
    "                    chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "\n",
    "                if [chunk_id, chunk_id_from_batch] in chunks_to_batches or [chunk_id_from_batch, chunk_id] in chunks_to_batches:\n",
    "                    print(chunk_id, chunk_id_from_batch, \"already in\")\n",
    "                \n",
    "                chunks_to_batches.append([chunk_id, chunk_id_from_batch])\n",
    "                chunks_to_batches.append([chunk_id_from_batch, chunk_id])\n",
    "                #chunks_to_batches[chunk_id] = chunk_id_from_batch # chunk e goes to chunk e' in batch b\n",
    "                #chunks_to_batches[chunk_id_from_batch] = chunk_id # add the symetric mapping\n",
    "\n",
    "                ent_len_to_chunk.pop(len_e) # pop one of the lengths\n",
    "                ent_chunk_to_len.pop(chunk_id) # pop the chunk from the entity and pop one of the lengths\n",
    "                batch_len_to_id.pop(len_b)\n",
    "                batch_id_to_len.pop(batch_id) # pop the batch and the length from the batch\n",
    "                break\n",
    "                \n",
    "    # ranmly match the rest of the chunks\n",
    "    for chunk_id, batch_id in zip(ent_chunk_to_len.keys(), batch_id_to_len.keys()):\n",
    "        if chunk_id not in chunks_to_batches:\n",
    "            chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "            while chunk_id_from_batch in blacklist:\n",
    "                    chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "                    \n",
    "            if [chunk_id, chunk_id_from_batch] in chunks_to_batches or [chunk_id_from_batch, chunk_id] in chunks_to_batches:\n",
    "                    print(chunk_id, chunk_id_from_batch, \"already in\")\n",
    "                \n",
    "            chunks_to_batches.append([chunk_id, chunk_id_from_batch])\n",
    "            chunks_to_batches.append([chunk_id_from_batch, chunk_id])\n",
    "            \n",
    "            #chunks_to_batches[chunk_id] = chunk_id_from_batch\n",
    "            #chunks_to_batches[chunk_id_from_batch] = chunk_id\n",
    "\n",
    "    return chunks_to_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mapping = []\n",
    "blacklist = set()\n",
    "for entity in disjoint_entities:\n",
    "    blacklist.update(entity['chunks'])\n",
    "\n",
    "for i, important_chunk in enumerate(disjoint_entities):\n",
    "    pts = all_injection_points_per_entity[important_chunk['entity_id']]\n",
    "\n",
    "    # The 'important_chunk' variable is the integer you need.\n",
    "    # Pass it directly to your function.\n",
    "    res = shloop(\n",
    "        pts,\n",
    "        important_chunk,\n",
    "        all_batches,\n",
    "        blacklist\n",
    "    )\n",
    "    \n",
    "    # add the chunks already in the injection mapping to a blacklist so they don't get sampled\n",
    "    blacklist.update(list(set([r[0] for r in res])))\n",
    "\n",
    "    # extend full mapping with the result\n",
    "    full_mapping.extend(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dict = {}\n",
    "for key, value in full_mapping:\n",
    "    normal_dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(normal_dict), len(full_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_chunks = sum(entity['num_chunks'] for entity in disjoint_entities)\n",
    "print(\"Combined amount of chunks in disjoint entities:\", total_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Track chunk_id -> list of entity_ids where it appears\n",
    "chunk_to_entities = defaultdict(set)\n",
    "\n",
    "for entity in disjoint_entities:\n",
    "    entity_id = entity['entity_id']\n",
    "    for chunk_id in entity['chunks']:\n",
    "        chunk_to_entities[chunk_id].add(entity_id)\n",
    "\n",
    "# Find chunk_ids that appear in more than one entity\n",
    "duplicate_chunks = {chunk_id: list(entity_ids) for chunk_id, entity_ids in chunk_to_entities.items() if len(entity_ids) > 1}\n",
    "\n",
    "print(f\"Number of chunk ids appearing in multiple entities: {len(duplicate_chunks)}\")\n",
    "for chunk_id, entity_ids in list(duplicate_chunks.items())[:-1]:  # show first 10 for brevity\n",
    "    print(f\"Chunk ID {chunk_id} appears in entities: {entity_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "shared_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for chunk_id, entities in chunk_to_entities.items():\n",
    "        # A chunk must be in at least two entities to be \"shared\".\n",
    "        if len(entities) < 2:\n",
    "            continue\n",
    "\n",
    "        # 2. Generate all unique pairs of entities for the current chunk.\n",
    "        # For a list [A, B, C], this yields (A, B), (A, C), (B, C).\n",
    "        for entity1, entity2 in itertools.combinations(entities, 2):\n",
    "            # 3. Increment the count for both directions of the pair.\n",
    "            shared_counts[entity1][entity2] += 1\n",
    "            shared_counts[entity2][entity1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp = {}\n",
    "# Sort entities by ID for consistent output order.\n",
    "for entity_id in sorted(shared_counts.keys()):\n",
    "    # Get the dictionary of shared counts for the current entity.\n",
    "    connections = shared_counts[entity_id]\n",
    "    \n",
    "    # Sort the connections by count (value) in descending order.\n",
    "    sorted_connections = dict(sorted(\n",
    "        connections.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    ))\n",
    "    opp[entity_id] = sorted_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for plotting: top N entities with most connections\n",
    "N = 20\n",
    "entity_ids = list(opp.keys())\n",
    "num_connections = [len(opp[eid]) for eid in entity_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_chunk_4026 = next((chunk for chunk in important_chunks if chunk['entity_id'] == 4026), None)\n",
    "print(important_chunk_4026)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count how many entities have each number of connections\n",
    "connection_counts = Counter(num_connections)\n",
    "\n",
    "# Prepare data for plotting\n",
    "x = sorted(connection_counts.keys())\n",
    "y = [connection_counts[k] for k in x]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, y)\n",
    "plt.xlabel(\"Number of Connections\")\n",
    "plt.ylabel(\"Number of Entities\")\n",
    "plt.title(\"Distribution of Entity Connections\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and select top N\n",
    "sorted_indices = sorted(range(len(num_connections)), key=lambda i: num_connections[i], reverse=True)[:N]\n",
    "top_entity_ids = [entity_ids[i] for i in sorted_indices]\n",
    "top_num_connections = [num_connections[i] for i in sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (entities)\n",
    "G.add_nodes_from(opp.keys())\n",
    "\n",
    "# Add edges with weights (number of shared chunks)\n",
    "for entity_id, connections in opp.items():\n",
    "    for other_id, weight in connections.items():\n",
    "        if weight > 1:\n",
    "            G.add_edge(entity_id, other_id, weight=weight)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(16, 12))\n",
    "pos = nx.spring_layout(G, k=0.15, seed=42)\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=100, node_color='skyblue')\n",
    "nx.draw_networkx_edges(G, pos, edgelist=edges, width=[w/2 for w in weights], alpha=0.7)\n",
    "#nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "# Remove node labels for a cleaner graph\n",
    "# (Do not call nx.draw_networkx_labels)\n",
    "plt.title(\"Entity Connection Graph (Edge weight = shared chunk count)\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count how many entities each chunk appears in\n",
    "chunk_entity_counts = [len(entity_ids) for entity_ids in duplicate_chunks.values()]\n",
    "\n",
    "# Count frequency of each \"number of entities\"\n",
    "chunk_count_freq = Counter(chunk_entity_counts)\n",
    "\n",
    "# Prepare data for plotting\n",
    "x_chunk = sorted(chunk_count_freq.keys())\n",
    "y_chunk = [chunk_count_freq[k] for k in x_chunk]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_chunk, y_chunk)\n",
    "plt.xlabel(\"Number of Entities a Chunk Appears In\")\n",
    "plt.ylabel(\"Number of Chunks\")\n",
    "plt.title(\"Distribution of Chunk Occurrences Across Entities\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(top_entity_ids)), top_num_connections, tick_label=top_entity_ids)\n",
    "plt.xlabel(\"Entity ID\")\n",
    "plt.ylabel(\"Number of Connections\")\n",
    "plt.title(\"Top Entities by Number of Connections\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_connections = np.mean(num_connections)\n",
    "print(\"Average number of connections per entity:\", average_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_connections = np.median(num_connections)\n",
    "print(\"Median number of connections per entity:\", median_connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entities_gt_10 = sum(1 for connections in opp.values() if len(connections) > 10)\n",
    "print(\"Number of entities sharing chunks with more than 10 connections:\", num_entities_gt_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 0\n",
    "for d in full_mapping:\n",
    "    total_len += len(d)\n",
    "\n",
    "total_len / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and load the swapping dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/joberant/NLP_2425b/yoavbaron/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(normal_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back later\n",
    "with open('/home/joberant/NLP_2425b/yoavbaron/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl', 'rb') as f:\n",
    "    swapping_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild dataset and dataloader with swapped chunk indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = TokenizerConfig.dolma2()\n",
    "tokenizer = HFTokenizer(\n",
    "            tokenizer_config.identifier,\n",
    "            pad_token_id=tokenizer_config.pad_token_id,\n",
    "            eos_token_id=tokenizer_config.eos_token_id,\n",
    "            bos_token_id=tokenizer_config.bos_token_id,\n",
    "        )\n",
    "\n",
    "include_instance_metadata = False # Set to true when you want tp retrieve metadata, during training set this to False\n",
    "work_dir = \"/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/hp_final/dataset-cache\"\n",
    "\n",
    "\n",
    "dataset_config = NumpyDatasetConfig.glob(\n",
    "    \"/home/morg/students/gottesman3/knowledge-analysis-suite/dolma/python/final_tokenizations_with_offsets/no_special/*.npy\",  # can be globs\n",
    "    name=NumpyDatasetType.kas_vsl,\n",
    "    max_sequence_length=2048,\n",
    "    min_sequence_length=64,\n",
    "    vsl_curriculum=VSLCurriculumConfig(name=VSLCurriculumType.grow_p2, num_cycles=8, balanced=False),\n",
    "    tokenizer=tokenizer_config,\n",
    "    work_dir=str(work_dir),\n",
    "    include_instance_metadata=include_instance_metadata,\n",
    "    swapping_dict = swapping_dict,\n",
    ")\n",
    "\n",
    "reordered_dataset = dataset_config.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_config = NumpyDataLoaderConfig(\n",
    "    global_batch_size=32768,\n",
    "    seed=0,\n",
    "    num_workers=8,\n",
    "    prefetch_factor = 16,\n",
    ")\n",
    "\n",
    "dataloader = data_loader_config.build(reordered_dataset)\n",
    "dataloader.reshuffle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_chunks = []\n",
    "for entity in disjoint_entities:\n",
    "    for chunk_id in entity['chunks']:\n",
    "        if chunk_id not in swapping_dict:\n",
    "            missing_chunks.append(chunk_id)\n",
    "\n",
    "print(f\"Number of missing important chunks: {len(missing_chunks)}\")\n",
    "if missing_chunks:\n",
    "    print(\"Missing chunk IDs:\", missing_chunks[:20])  # show first 20 for brevity\n",
    "else:\n",
    "    print(\"All important chunks are present in swapping_dict.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = list(swapping_dict.keys())\n",
    "sorted_keys.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    if i == inj[0]:\n",
    "        print(len(batch['index']), batch['index'])\n",
    "        break\n",
    "    if i > max(inj):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = [ 7631809,  6940641,  3542626,  9931434,  9470964,  5211576,  4257442,\n",
    "          109997, 10159240,  2756754,  8779079,  2222801,  4308700,  1495528,\n",
    "         2898556,  2697159, 10461777,   136076,  5838211,  7914579,  3234602,\n",
    "         2059624,  9026918,  6059905,  7357863,  3345625,   818789,  4348662,\n",
    "         2368892,  9938633,   264110, 10054712,  1550785,  2556067,   235054,\n",
    "         7588519,  1821616,  8467303,  5276389,   273819,  6845202,  8054962,\n",
    "           45173,  1475641, 10126768,  6707865,  8668042,  9398828,  2105198,\n",
    "         5300031,  7318456,  3294546,  8825413,  5412833,   362726, 10035273,\n",
    "         7699557,  7837840,  8854210,  9661840,  6383184,   206394,  6719335,\n",
    "        10423236,  8018408,  5597199,  5276516,  5231316,  2196400, 10267688,\n",
    "         1521057,  4652225,  9679001,  9652065,  9429994,  2282572,  6956468,\n",
    "         8611192,  7721421,  6933702,  4336640,  6175126,  8009534,  3572447,\n",
    "         5267884,  1575983,  3548330,  7403549,  6448858,  1431713,   827523,\n",
    "         6320696,  3430047,  5381881,  9632193,  9383363,  1606688, 10108639,\n",
    "         3705895, 10258338,  9717646,  6062364,   482440,  7780681,  1045536,\n",
    "         8046308,  2504279,  3943587,  2441651,  3855765,  4084704,  6087419,\n",
    "         4892372,  5649860,  6118288,   750845,  8505029,  9260357,  9635432,\n",
    "         4270765, 10089093, 10223830,  3740823,  8578604,  3763499,  9205088,\n",
    "         9813411,  6419315,  6763331,  9119661,  9088378,  5766675,  9326163,\n",
    "         7975639,  1991544,   611044,  3355656,  2055100,  9704132,  6231878,\n",
    "         2614424,  2053407,  8520540,   883889,  2137193,  7605850,    57965,\n",
    "         6222716,  3650282,  5265742,  4984812, 10008793,  8164404,  5001298,\n",
    "         4981525,  5767203,   695583,  6776778,  9820583,   711651,  4299634,\n",
    "          593114,  5222237,  9881415,  7831993,  4950886,  5730604,  2090383,\n",
    "         8206375,  6587449,  2826392,  4036001,  4466790,  5539846,  9809955,\n",
    "         5364018,  3159089,  4362151,  5900454,  6523105,  8326342,  8936973,\n",
    "         6565883,  8906095,  8984150,   541885,  4060055,  2743755,  7070246,\n",
    "         9201059, 10430806,  1613698,  1398794,  6782600,  5851072,  9825166,\n",
    "         7021826,  1494532,  8008803,  2901043,  9942393,  8044859,    35542,\n",
    "         1997732,  9884157,  7442658, 10166251,  5547095,   561454,  2844523,\n",
    "           25535,  7805619,  6612396,   606006,  2505521,  8799493,  1906972,\n",
    "         7157075,  6700643,  4498508,  9373815,  5539025,  2517344,  5399511,\n",
    "         5084722,  2777578,  5642244,  9995602,  9493457,  2310500,  4885621,\n",
    "         2002396,  5051772,  9958852,  3151452,  4636379,  6961252,  5910843,\n",
    "         6056337,  6797837,   739182,  6538852,  5331249,  5909444,  5970866,\n",
    "           73284,  6716057,  7959418,  4999099,  6817239,  8087901,  6826491,\n",
    "         5403531,  1104048,  4690218,  3440652]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swp = [[ 5303980,  3401112,  2619408,  7631809,  6940641,  3542626,  9931434,\n",
    "         9470964,  5211576,  4257442,   109997, 10159240,  2756754,  8779079,\n",
    "         2222801,  4308700,  1495528,  2898556,  2697159, 10461777,   136076,\n",
    "         5838211,  7914579,  3234602,  2059624,  9026918,  6059905,  7357863,\n",
    "         3345625,   818789,  4348662,  2368892,  9938633,   264110, 10054712,\n",
    "         1550785,  2556067,   235054,  7588519,  1821616,  8467303,  5276389,\n",
    "          273819,  6845202,  8054962,    45173,  1475641, 10126768,  6707865,\n",
    "         8668042,  9398828,  2105198,  5300031,  7318456,  3294546,  8825413,\n",
    "         5412833,   362726, 10035273,  7699557,  7837840,  8854210,  9661840,\n",
    "         6383184,   206394,  6719335, 10423236,  8018408,  5597199,  5276516,\n",
    "         5231316,  2196400, 10267688,  1521057,  4652225,  9679001,  9652065,\n",
    "         9429994,  2282572,  6956468,  8611192,  7721421,  6933702,  4336640,\n",
    "         6175126,  8009534,  3572447,  5267884,  1575983,  3548330,  7403549,\n",
    "         6448858,  1431713,   827523,  6320696,  3430047,  5381881,  9632193,\n",
    "         9383363,  1606688, 10108639,  3705895, 10258338,  9717646,  6062364,\n",
    "          482440,  7780681,  1045536,  8046308,  2504279,  3943587,  2441651,\n",
    "         3855765,  4084704,  6087419,  4892372,  5649860,  6118288,   750845,\n",
    "         8505029,  9260357,  9635432,  4270765, 10089093, 10223830,  3740823,\n",
    "         8578604,  3763499,  9205088,  9813411,  6419315,  6763331,  9119661,\n",
    "         9088378,  5766675,  9326163,  7975639,  1991544,   611044,  3355656,\n",
    "         2055100,  9704132,  6231878,  2614424,  2053407,  8520540,   883889,\n",
    "         2137193,  7605850,    57965,  6222716,  3650282,  5265742,  4984812,\n",
    "        10008793,  8164404,  5001298,  4981525,  5767203,   695583,  6776778,\n",
    "         9820583,   711651,  4299634,   593114,  5222237,  9881415,  7831993,\n",
    "         4950886,  5730604,  2090383,  8206375,  6587449,  2826392,  4036001,\n",
    "         4466790,  5539846,  9809955,  5364018,  3159089,  4362151,  5900454,\n",
    "         6523105,  8326342,  8936973,  6565883,  8906095,  8984150,   541885,\n",
    "         4060055,  2743755,  7070246,  9201059, 10430806,  1613698,  1398794,\n",
    "         6782600,  5851072,  9825166,  7021826,  1494532,  8008803,  2901043,\n",
    "         9942393,  8044859,    35542,  1997732,  9884157,  7442658, 10166251,\n",
    "         5547095,   561454,  2844523,    25535,  7805619,  6612396,   606006,\n",
    "         2505521,  8799493,  1906972,  7157075,  6700643,  4498508,  9373815,\n",
    "         5539025,  2517344,  5399511,  5084722,  2777578,  5642244,  9995602,\n",
    "         9493457,  2310500,  4885621,  2002396,  5051772,  9958852,  3151452]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten swp if it's a list of lists\n",
    "swp_flat = swp[0] if isinstance(swp[0], (list, np.ndarray)) else swp\n",
    "\n",
    "# Find elements in orig not in swp_flat\n",
    "orig_not_in_swp = [x for x in orig if x not in swp_flat]\n",
    "# Find elements in swp_flat not in orig\n",
    "swp_not_in_orig = [x for x in swp_flat if x not in orig]\n",
    "\n",
    "print(\"In orig but not in swp:\", orig_not_in_swp)\n",
    "print(\"In swp but not in orig:\", swp_not_in_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2-olmo-2-copy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
