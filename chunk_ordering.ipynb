{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "# Add the src directory to Python path\n",
    "olmo_core_path = Path.cwd() / \"src\"\n",
    "if olmo_core_path.exists():\n",
    "    sys.path.insert(0, str(olmo_core_path))\n",
    "\n",
    "from olmo_core.data import (\n",
    "    NumpyDataLoaderConfig,\n",
    "    NumpyDatasetConfig,\n",
    "    NumpyDatasetType,\n",
    "    TokenizerConfig,\n",
    ")\n",
    "from olmo_core.data.numpy_dataset import (\n",
    "    VSLCurriculumType,\n",
    "    VSLCurriculumConfig,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your new cache base directory (change this to your preferred location)\n",
    "cache_base = \"/home/joberant/NLP_2425b/shirab6\"\n",
    "\n",
    "# Set all relevant Hugging Face cache directories\n",
    "os.environ[\"HF_HOME\"] = cache_base\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(cache_base, \"transformers\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = os.path.join(cache_base, \"datasets\")\n",
    "os.environ[\"HF_TOKENIZERS_CACHE\"] = os.path.join(cache_base, \"tokenizers\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from olmo_eval import HFTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = TokenizerConfig.dolma2()\n",
    "tokenizer = HFTokenizer(\n",
    "            tokenizer_config.identifier,\n",
    "            pad_token_id=tokenizer_config.pad_token_id,\n",
    "            eos_token_id=tokenizer_config.eos_token_id,\n",
    "            bos_token_id=tokenizer_config.bos_token_id,\n",
    "        )\n",
    "\n",
    "include_instance_metadata = False # Set to true when you want tp retrieve metadata, during training set this to False\n",
    "work_dir = \"/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/hp_final/dataset-cache\"\n",
    "\n",
    "dataset_config = NumpyDatasetConfig.glob(\n",
    "    \"/home/morg/students/gottesman3/knowledge-analysis-suite/dolma/python/final_tokenizations_with_offsets/no_special/*.npy\",  # can be globs\n",
    "    name=NumpyDatasetType.kas_vsl,\n",
    "    max_sequence_length=2048,\n",
    "    min_sequence_length=64,\n",
    "    vsl_curriculum=VSLCurriculumConfig(name=VSLCurriculumType.grow_p2, num_cycles=8, balanced=False),\n",
    "    tokenizer=tokenizer_config,\n",
    "    work_dir=str(work_dir),\n",
    "    include_instance_metadata=include_instance_metadata,\n",
    ")\n",
    "kas_dataset = dataset_config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_config = NumpyDataLoaderConfig(\n",
    "    global_batch_size=32768,\n",
    "    seed=0,\n",
    "    num_workers=8,\n",
    "    prefetch_factor = 16,\n",
    ")\n",
    "\n",
    "dataloader = data_loader_config.build(kas_dataset)\n",
    "dataloader.reshuffle(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load PopQA dataset and filter entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_chunks(dataset, min_num_chunks, max_num_chunks, instance_lengths):\n",
    "    # Filter the dataset\n",
    "    filtered_dataset = dataset['train'].filter(\n",
    "        lambda example: min_num_chunks <= example['subject_num_chunks'] <= max_num_chunks\n",
    "    )   \n",
    "\n",
    "    # Create list of dictionaries with subject info and chunk lengths\n",
    "    result_list = []    \n",
    "\n",
    "    for example in filtered_dataset:\n",
    "        \n",
    "        subject_name = example['subj']\n",
    "        subject_id = example['subj_id']\n",
    "        chunks = example['subject_chunks']\n",
    "        num_chunks = example['subject_num_chunks']\n",
    "\n",
    "        chunk_lengths = instance_lengths[chunks]\n",
    "\n",
    "        if subject_name == 'Madison':\n",
    "            print(chunks)\n",
    "\n",
    "        # Sort chunks by their lengths (descending order)\n",
    "        if len(chunk_lengths) > 0:\n",
    "            # Create pairs of (chunk, length) and sort by length\n",
    "            chunk_length_pairs = list(zip(chunks, chunk_lengths))\n",
    "            chunk_length_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Separate back into sorted chunks and lengths\n",
    "            sorted_chunks = [pair[0] for pair in chunk_length_pairs]\n",
    "            sorted_lengths = [pair[1] for pair in chunk_length_pairs]\n",
    "        else:\n",
    "            sorted_chunks = chunks\n",
    "            sorted_lengths = chunk_lengths\n",
    "\n",
    "        subject_dict = {\n",
    "            'entity_id': subject_id,\n",
    "            'num_chunks': num_chunks,\n",
    "            'chunks': sorted_chunks,\n",
    "            'chunks_lengths': sorted_lengths\n",
    "        }\n",
    "        \n",
    "        result_list.append(subject_dict)    \n",
    "\n",
    "    # Sort the list by number of chunks (descending order)\n",
    "    result_list.sort(key=lambda x: x['num_chunks'], reverse=True)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"dhgottesman/popqa-kas\")\n",
    "\n",
    "\"\"\"\n",
    "importsnt chunks has the following structure:\n",
    "        {\n",
    "            'entity_id': subject_id,\n",
    "            'num_chunks': num_chunks,\n",
    "            'chunks': sorted_chunks,\n",
    "            'chunks_lengths': sorted_lengths\n",
    "        }\n",
    "\"\"\"\n",
    "important_chunks = get_important_chunks(ds, 50, 100, kas_dataset.get_instance_lengths())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load original batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batches = np.load(\"/home/morg/students/gottesman3/knowledge-analysis-suite/OLMo-core/batch_indices.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample injection points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_injection_points(total_steps, num_points_to_sample, max_num_chunks, interval, seed=None):\n",
    "    \"\"\"\n",
    "    Samples unique injection points from a valid starting range to avoid overflow \n",
    "    when assigning chunk indices.\n",
    "\n",
    "    Args:\n",
    "        total_steps (int): The maximum possible step value (exclusive upper bound).\n",
    "        num_points_to_sample (int): Number of injection points to sample.\n",
    "        max_num_chunks (int): Maximum num_chunks across all entities.\n",
    "        interval (int): Distance between chunk indices.\n",
    "        seed (int, optional): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: Sorted list of valid injection starting points.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    max_valid_start = total_steps - (max_num_chunks - 1) * interval\n",
    "    if max_valid_start <= 0:\n",
    "        raise ValueError(\"Interval and chunk size too large for total steps.\")\n",
    "\n",
    "    if num_points_to_sample > max_valid_start:\n",
    "        raise ValueError(\"Cannot sample more injection points than available valid start points.\")\n",
    "\n",
    "    sampled_points = random.sample(range(max_valid_start), k=num_points_to_sample)\n",
    "    return sorted(sampled_points)\n",
    "\n",
    "\n",
    "def assign_indices_to_entities(entities, injection_points, interval):\n",
    "    \"\"\"\n",
    "    Assigns indices to each entity starting at a given injection point with spacing.\n",
    "\n",
    "    Args:\n",
    "        entities (List[dict]): List of entity dicts.\n",
    "        injection_points (List[int]): List of sampled injection start points.\n",
    "        interval (int): Distance between chunk indices.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[int]]: Mapping from entity name to list of indices.\n",
    "    \"\"\"\n",
    "    if len(entities) != len(injection_points):\n",
    "        raise ValueError(\"Number of entities must match number of injection points.\")\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for entity, start in zip(entities, injection_points):\n",
    "        entity_id = entity['entity_id']\n",
    "        num_chunks = entity['num_chunks']\n",
    "        indices = [start + i * interval for i in range(num_chunks)]\n",
    "        result[entity_id] = indices\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 1\n",
    "\n",
    "total_number_of_batches = dataloader.total_batches\n",
    "injection_points = sample_injection_points(total_number_of_batches, len(important_chunks), 100, interval, 0)\n",
    "all_injection_points_per_entity = assign_indices_to_entities(important_chunks, injection_points, interval)\n",
    "all_injection_points_per_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in important_chunks:\n",
    "    if len(all_injection_points_per_entity[entity['entity_id']]) != len(entity['chunks']):\n",
    "        print(f'Found a problem in entity {entity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for entity in important_chunks:\n",
    "    for index, chunk in enumerate(entity['chunks']):\n",
    "        pairs.append((chunk, np.random.choice(all_batches[all_injection_points_per_entity[entity['entity_id']][index]])))\n",
    "\n",
    "print('Length of pairs: ', len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "keys = []\n",
    "for i, j in pairs:\n",
    "    keys.extend([i, j])\n",
    "\n",
    "print(Counter(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "conflicting_pairs = []\n",
    "\n",
    "seen_keys = set()\n",
    "\n",
    "for i, j in pairs:\n",
    "    for k, v in [(i, j), (j, i)]:\n",
    "        if k in d:\n",
    "            conflicting_pairs.append(((i, j), k, d[k], v))  # store the original pair, conflicting key, old value, new value\n",
    "        d[k] = v\n",
    "\n",
    "# Print conflicts\n",
    "for pair, key, old_val, new_val in conflicting_pairs:\n",
    "    print(f\"Conflict from pair {pair}: key {key} was {old_val}, overwritten with {new_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "swapping_dict = {}\n",
    "for (i, j) in pairs:\n",
    "    swapping_dict[i] = j\n",
    "    swapping_dict[j] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(swapping_dict) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swapping_dict = {}\n",
    "list_of_pairs = []\n",
    "for entity in important_chunks:\n",
    "    pairs = list(zip(entity['chunks'], np.random.choice(all_batches[all_injection_points_per_entity[entity['entity_id']]])))\n",
    "    list_of_pairs.append(pairs)\n",
    "    # Add both directions for each pair\n",
    "    for original, replacement in pairs:\n",
    "        swapping_dict[original] = replacement\n",
    "        swapping_dict[replacement] = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = 0\n",
    "for pairs in list_of_pairs:\n",
    "    all_pairs += len(pairs)\n",
    "\n",
    "all_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Swapping Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shloop(\n",
    "    injection_points: List[int],\n",
    "    entity_data: dict,\n",
    "    batch_to_chunks_map: dict,\n",
    ") -> List[List]:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # 1. Get entity chunks available for swapping and their lengths\n",
    "\n",
    "    ent_chunk_to_len = dict(zip(entity_data['chunks'], entity_data['chunks_lengths']))\n",
    "    ent_len_to_chunk = {v: k for k, v in ent_chunk_to_len.items()}\n",
    "\n",
    "    # casting to int but might want to edit this\n",
    "    batch_id_to_len = {}\n",
    "    batch_len_to_id = {}\n",
    "    for batch in injection_points:\n",
    "        batch_len = int(32768 / len(batch_to_chunks_map[batch]))\n",
    "        batch_id_to_len[batch] = batch_len\n",
    "        batch_len_to_id[batch_len] = batch\n",
    "\n",
    "    # 2. Calculate the injection span\n",
    "    num_chunks = len(entity_data['chunks'])\n",
    "    #print(f\"Injection span: {list(injection_points)}\")\n",
    "    if len(injection_points) != num_chunks:\n",
    "        f\"Entity {entity_data['entity_id']} expected {num_chunks} injection points, but got {len(injection_points)}.\"\n",
    "    \n",
    "\n",
    "    sb = sorted(batch_len_to_id.keys())   \n",
    "    se = sorted(ent_len_to_chunk.keys())\n",
    "\n",
    "    chunks_to_batches = []\n",
    "    for len_e in se:\n",
    "        for len_b in sb:\n",
    "            if len_b == len_e:\n",
    "                #print(len_e, len_b)\n",
    "                chunk_id = ent_len_to_chunk[len_e]\n",
    "                batch_id = batch_len_to_id[len_b]\n",
    "\n",
    "                #print(f\"Chunk {chunk_id} with length {len_e} will be swapped with batch {batch_id} with length {len_b}\")\n",
    "                # get a random chunk id from the batch\n",
    "                chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "\n",
    "                if [chunk_id, chunk_id_from_batch] in chunks_to_batches or [chunk_id_from_batch, chunk_id] in chunks_to_batches:\n",
    "                    print(chunk_id, chunk_id_from_batch, \"already in\")\n",
    "                \n",
    "                chunks_to_batches.append([chunk_id, chunk_id_from_batch])\n",
    "                chunks_to_batches.append([chunk_id_from_batch, chunk_id])\n",
    "                #chunks_to_batches[chunk_id] = chunk_id_from_batch # chunk e goes to chunk e' in batch b\n",
    "                #chunks_to_batches[chunk_id_from_batch] = chunk_id # add the symetric mapping\n",
    "\n",
    "                ent_len_to_chunk.pop(len_e) # pop one of the lengths\n",
    "                ent_chunk_to_len.pop(chunk_id) # pop the chunk from the entity and pop one of the lengths\n",
    "                batch_len_to_id.pop(len_b)\n",
    "                batch_id_to_len.pop(batch_id) # pop the batch and the length from the batch\n",
    "                break\n",
    "                \n",
    "    # ranmly match the rest of the chunks\n",
    "    for chunk_id, batch_id in zip(ent_chunk_to_len.keys(), batch_id_to_len.keys()):\n",
    "        if chunk_id not in chunks_to_batches:\n",
    "            chunk_id_from_batch = random.choice(batch_to_chunks_map[batch_id])\n",
    "\n",
    "            if [chunk_id, chunk_id_from_batch] in chunks_to_batches or [chunk_id_from_batch, chunk_id] in chunks_to_batches:\n",
    "                    print(chunk_id, chunk_id_from_batch, \"already in\")\n",
    "                \n",
    "            chunks_to_batches.append([chunk_id, chunk_id_from_batch])\n",
    "            chunks_to_batches.append([chunk_id_from_batch, chunk_id])\n",
    "            \n",
    "            #chunks_to_batches[chunk_id] = chunk_id_from_batch\n",
    "            #chunks_to_batches[chunk_id_from_batch] = chunk_id\n",
    "\n",
    "    return chunks_to_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mapping = []\n",
    "for i, important_chunk in enumerate(important_chunks):\n",
    "    pts = all_injection_points_per_entity[important_chunk['entity_id']]\n",
    "\n",
    "    # The 'important_chunk' variable is the integer you need.\n",
    "    # Pass it directly to your function.\n",
    "    res = shloop(\n",
    "        pts,\n",
    "        important_chunk,\n",
    "        all_batches\n",
    "    )\n",
    "    # extend full mapping with the result\n",
    "    full_mapping.extend(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dict = {}\n",
    "for key, value in full_mapping:\n",
    "    if key not in grouped_dict:\n",
    "        grouped_dict[key] = []\n",
    "    grouped_dict[key].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(grouped_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it back later\n",
    "with open('/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/swapping_dict.pkl', 'rb') as f:\n",
    "    swapping_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild dataset and dataloader with swapped chunk indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = TokenizerConfig.dolma2()\n",
    "tokenizer = HFTokenizer(\n",
    "            tokenizer_config.identifier,\n",
    "            pad_token_id=tokenizer_config.pad_token_id,\n",
    "            eos_token_id=tokenizer_config.eos_token_id,\n",
    "            bos_token_id=tokenizer_config.bos_token_id,\n",
    "        )\n",
    "\n",
    "include_instance_metadata = False # Set to true when you want tp retrieve metadata, during training set this to False\n",
    "work_dir = \"/home/joberant/NLP_2425b/shirab6/knowledge-analysis-suite/OLMo-core/hp_final/dataset-cache\"\n",
    "\n",
    "\n",
    "dataset_config = NumpyDatasetConfig.glob(\n",
    "    \"/home/morg/students/gottesman3/knowledge-analysis-suite/dolma/python/final_tokenizations_with_offsets/no_special/*.npy\",  # can be globs\n",
    "    name=NumpyDatasetType.kas_vsl,\n",
    "    max_sequence_length=2048,\n",
    "    min_sequence_length=64,\n",
    "    vsl_curriculum=VSLCurriculumConfig(name=VSLCurriculumType.grow_p2, num_cycles=8, balanced=False),\n",
    "    tokenizer=tokenizer_config,\n",
    "    work_dir=str(work_dir),\n",
    "    include_instance_metadata=include_instance_metadata,\n",
    "    swapping_dict = swapping_dict,\n",
    ")\n",
    "\n",
    "reordered_dataset = dataset_config.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_config = NumpyDataLoaderConfig(\n",
    "    global_batch_size=32768,\n",
    "    seed=0,\n",
    "    num_workers=8,\n",
    "    prefetch_factor = 16,\n",
    ")\n",
    "\n",
    "dataloader = data_loader_config.build(reordered_dataset)\n",
    "dataloader.reshuffle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys = list(swapping_dict.keys())\n",
    "sorted_keys.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    if i == 44:\n",
    "        print(batch)\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2-olmo-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
